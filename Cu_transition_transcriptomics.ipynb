{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "import re #regular expression matching for removing unwanted columns by name \n",
    "import natsort as ns #3rd party package for natural sorting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raw_data_cleanup(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports RNAseq .csv file and does basic clean up of \"FM40\" \n",
    "        -sorts FM40 timecourse sequence chronologically\n",
    "        -removes all QC data and non FM40 columns\n",
    "        -returns dataframe with locus tag set as index\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"{} was located in the directory\".format(filename))\n",
    "                \n",
    "#import the data\n",
    "        data0_raw = pd.read_csv(filename, sep = \"\\t\") \n",
    "        print(\"{} was imported into dataframe\".format(filename))\n",
    "        \n",
    "#removing all QC data\n",
    "        data1_noQC = data0_raw.select(lambda x: not re.search(\"QC\", x), axis = 1) \n",
    "        print(\"QC columns were removed from dataframe\")\n",
    "\n",
    "#removing all non FM40 data\n",
    "        data2_FM40only = data1_noQC.select(lambda x: re.search(\"FM40\", x), axis = 1)\n",
    "        print(\"All non FM40 data were removed from dataframe\")\n",
    "        \n",
    "#naturally sorting FM40 data by columns \n",
    "        cols = list(ns.natsorted(data2_FM40only.columns))\n",
    "        data3_sorted=data2_FM40only[cols]\n",
    "        print(\"All FM40 columns were sorted by timecourse sequence\")\n",
    "        \n",
    "#adding the descriptor columns back to FM40\n",
    "        qualitative = data0_raw.loc[:,\"locus_tag\":\"translation\"]\n",
    "        data4_sorted = pd.concat([qualitative, data3_sorted], axis = 1)\n",
    "\n",
    "#setting locus tag to be the index\n",
    "        data5_index = data4_sorted.set_index(\"locus_tag\")\n",
    "        \n",
    "    \n",
    "        print(\"Clean-up of raw data complete\")\n",
    "        return data5_index\n",
    "        \n",
    "    else:\n",
    "        print(\"{} does not exist in directory. Function was not complete.\".format(filename))\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TPM counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TPM_counts(dataframe, \n",
    "              gene_start,\n",
    "              gene_stop,\n",
    "              columns):\n",
    "    \n",
    "    \"\"\"\n",
    "    TPM_counts(dataframe, gene_start, gene_stop, columns):\n",
    "\n",
    "    returns a dataframe with TPM instead of reads\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    daraframe = dataframe object variable\n",
    "    gene_start = string with column name containing gene start coordinate\n",
    "    gene_stop = string with column name containing gene stop coordinate\n",
    "    columns = list of strings of column names to be converted to TPM\n",
    "\n",
    "\n",
    "    Run the following two lines to properly execute this function:\n",
    "\n",
    "    columns = ['5GB1_FM40_T0m_TR2', '5GB1_FM40_T10m_TR3', '5GB1_FM40_T20m_TR2', '5GB1_FM40_T40m_TR1',\n",
    "           '5GB1_FM40_T60m_TR1', '5GB1_FM40_T90m_TR2', '5GB1_FM40_T150m_TR1_remake', '5GB1_FM40_T180m_TR1']\n",
    "\n",
    "    TPM_counts(df,\"start_coord\",\"end_coord\",columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    #create empty dataframe \n",
    "    gene_length = pd.DataFrame()\n",
    "    \n",
    "    #gene length in kilo base pairs as new column\n",
    "    gene_length[\"gene_length\"] = (dataframe[gene_stop]- dataframe[gene_start] + 1)/1000   \n",
    "    \n",
    "    #normalize read counts by gene length in kilo base pairs\n",
    "    RPK = dataframe.loc[:,columns].div(gene_length.gene_length, axis=0) \n",
    "    \n",
    "    #creating a series with the sums of each FM40 column / 1,000,000\n",
    "    norm_sum = RPK.sum(axis=0)/1000000 \n",
    "    norm_sum1 = pd.Series.to_frame(norm_sum)\n",
    "    norm_sum2 = norm_sum1.T\n",
    "    \n",
    "    #dividing by the the total transcript counts in each repicate\n",
    "    TPM = RPK.div(norm_sum2.ix[0]) \n",
    "    \n",
    "    dataframe.loc[:,columns] = TPM\n",
    "    \n",
    "    return dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating congruency table - pearson correlation across every pair of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def congruency_table(df, \n",
    "         data_clm_strt, \n",
    "         data_clm_stop, \n",
    "         step, \n",
    "         mask_diagonal=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    corr(df, data_clm_strt, data_clm_stop, step = len(df.columns), mask_diagonal=False)\n",
    "    \n",
    "    returns a new datafram - congruency table - a pairwise pearson correlation matrix for every row pair\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df - dataframe argument - recommended to use TPM counts for RNAseq datasets. \n",
    "    data_clm_strt = first column that contains data to be processed \n",
    "    data_clm_stop = last column that contains data to be processed\n",
    "    step = length of dataset \n",
    "    mask_diagonal = mask diagonal values which shoud come out as 1\n",
    "    \n",
    "    \n",
    "    Run the following lines to execute the function for my data\n",
    "    \n",
    "    congruency_table(df1, \"5GB1_FM40_T0m_TR2\" , \"5GB1_FM40_T180m_TR1\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.loc[:, data_clm_strt: data_clm_stop] #isolating the rows that are relavent to us. \n",
    "    df = df.T \n",
    "    \n",
    "    n = df.shape[0]\n",
    "\n",
    "    def corr_closure(df):\n",
    "        d = df.values\n",
    "        sums = d.sum(0, keepdims=True)\n",
    "        stds = d.std(0, keepdims=True)\n",
    "\n",
    "        def corr_(k=0, l=10):\n",
    "            d2 = d.T.dot(d[:, k:l])\n",
    "            sums2 = sums.T.dot(sums[:, k:l])\n",
    "            stds2 = stds.T.dot(stds[:, k:l])\n",
    "\n",
    "            return pd.DataFrame((d2 - sums2 / n) / stds2 / n,\n",
    "                                df.columns, df.columns[k:l])\n",
    "\n",
    "        return corr_\n",
    "\n",
    "    c = corr_closure(df)\n",
    "\n",
    "    step = min(step, df.shape[1])\n",
    "\n",
    "    tups = zip(range(0, n, step), range(step, n + step, step))\n",
    "\n",
    "    corr_table = pd.concat([c(*t) for t in tups], axis=1)\n",
    "\n",
    "    if mask_diagonal:\n",
    "        np.fill_diagonal(corr_table.values, np.nan)\n",
    "\n",
    "    return corr_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to tie it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5G_counts.tsv was located in the directory\n",
      "5G_counts.tsv was imported into dataframe\n",
      "QC columns were removed from dataframe\n",
      "All non FM40 data were removed from dataframe\n",
      "All FM40 columns were sorted by timecourse sequence\n",
      "Clean-up of raw data complete\n",
      "The shape of the TPM table is  (4593, 16)\n",
      "The shape of the congruency table is  (4593, 4593)\n"
     ]
    }
   ],
   "source": [
    "df1_raw_FM40 = raw_data_cleanup(\"5G_counts.tsv\")\n",
    "\n",
    "\n",
    "columns = ['5GB1_FM40_T0m_TR2', '5GB1_FM40_T10m_TR3', '5GB1_FM40_T20m_TR2', '5GB1_FM40_T40m_TR1',\n",
    "           '5GB1_FM40_T60m_TR1', '5GB1_FM40_T90m_TR2', '5GB1_FM40_T150m_TR1_remake', '5GB1_FM40_T180m_TR1']\n",
    "\n",
    "df2_TPM = TPM_counts(df1_raw_FM40, \"start_coord\", \"end_coord\",columns)\n",
    "\n",
    "df3_matrix = congruency_table(df2_TPM, \"5GB1_FM40_T0m_TR2\" , \"5GB1_FM40_T180m_TR1\", step = df2_TPM.shape[0])\n",
    "\n",
    "print(\"The shape of the TPM table is \", df2_TPM.shape)\n",
    "print(\"The shape of the congruency table is \", df3_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
