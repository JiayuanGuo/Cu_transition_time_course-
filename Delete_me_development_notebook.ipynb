{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import re #regular expression matching for removing unwanted columns by name \n",
    "import natsort as ns #3rd party package for natural sorting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_data_cleanup(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports RNAseq .csv file and does basic clean up of \"FM40\" \n",
    "        -sorts FM40 timecourse sequence chronologically\n",
    "        -removes all QC data and non FM40 columns\n",
    "        -returns dataframe with locus tag set as index\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"{} was located in the directory\".format(filename))\n",
    "                \n",
    "#import the data\n",
    "        data0_raw = pd.read_csv(filename, sep = \"\\t\") \n",
    "        print(\"{} was imported into dataframe\".format(filename))\n",
    "        \n",
    "#removing all QC data\n",
    "        data1_noQC = data0_raw.select(lambda x: not re.search(\"QC\", x), axis = 1) \n",
    "        print(\"QC columns were removed from dataframe\")\n",
    "\n",
    "#removing all non FM40 data\n",
    "        data2_FM40only = data1_noQC.select(lambda x: re.search(\"FM40\", x), axis = 1)\n",
    "        print(\"All non FM40 data were removed from dataframe\")\n",
    "        \n",
    "#naturally sorting FM40 data by columns \n",
    "        cols = list(ns.natsorted(data2_FM40only.columns))\n",
    "        data3_sorted=data2_FM40only[cols]\n",
    "        print(\"All FM40 columns were sorted by timecourse sequence\")\n",
    "        \n",
    "#adding the descriptor columns back to FM40\n",
    "        qualitative = data0_raw.loc[:,\"locus_tag\":\"translation\"]\n",
    "        data4_sorted = pd.concat([qualitative, data3_sorted], axis = 1)\n",
    "\n",
    "#setting locus tag to be the index\n",
    "        data5_index = data4_sorted.set_index(\"locus_tag\")\n",
    "        \n",
    "    \n",
    "        print(\"Clean-up of raw data complete\")\n",
    "        return data5_index\n",
    "        \n",
    "    else:\n",
    "        print(\"{} does not exist in directory. Function was not complete.\".format(filename))\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TPM_counts(dataframe, \n",
    "              gene_start,\n",
    "              gene_stop,\n",
    "              columns):\n",
    "    \n",
    "    \"\"\"\n",
    "    TPM_counts(dataframe, gene_start, gene_stop, columns):\n",
    "\n",
    "    returns a dataframe with TPM instead of reads\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    daraframe = dataframe object variable\n",
    "    gene_start = string with column name containing gene start coordinate\n",
    "    gene_stop = string with column name containing gene stop coordinate\n",
    "    columns = list of strings of column names to be converted to TPM\n",
    "\n",
    "\n",
    "    Run the following two lines to properly execute this function:\n",
    "\n",
    "    columns = ['5GB1_FM40_T0m_TR2', '5GB1_FM40_T10m_TR3', '5GB1_FM40_T20m_TR2', '5GB1_FM40_T40m_TR1',\n",
    "           '5GB1_FM40_T60m_TR1', '5GB1_FM40_T90m_TR2', '5GB1_FM40_T150m_TR1_remake', '5GB1_FM40_T180m_TR1']\n",
    "\n",
    "    TPM_counts(df,\"start_coord\",\"end_coord\",columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    #create empty dataframe \n",
    "    gene_length = pd.DataFrame()\n",
    "    \n",
    "    #gene length in kilo base pairs as new column\n",
    "    gene_length[\"gene_length\"] = (dataframe[gene_stop]- dataframe[gene_start] + 1)/1000   \n",
    "    \n",
    "    #normalize read counts by gene length in kilo base pairs\n",
    "    RPK = dataframe.loc[:,columns].div(gene_length.gene_length, axis=0) \n",
    "    \n",
    "    #creating a series with the sums of each FM40 column / 1,000,000\n",
    "    norm_sum = RPK.sum(axis=0)/1000000 \n",
    "    norm_sum1 = pd.Series.to_frame(norm_sum)\n",
    "    norm_sum2 = norm_sum1.T\n",
    "    \n",
    "    #dividing by the the total transcript counts in each repicate\n",
    "    TPM = RPK.div(norm_sum2.ix[0]) \n",
    "    \n",
    "    dataframe.loc[:,columns] = TPM\n",
    "    \n",
    "    return dataframe \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log2 fold transfrom of the TPM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_2_transform(dataframe,\n",
    "                    first_data_column,\n",
    "                    last_data_column):\n",
    "                  \n",
    "    \"\"\"\n",
    "    log_2_transform(dataframe, \n",
    "                    first_data_column, \n",
    "                    last_data_column)\n",
    "    \n",
    "    Return a new dataframe with the range of data columns log2 transformed. \n",
    "    *all zero values are changed to 1 (yield 0 after transform)\n",
    "    *all values less than 1 are changed to 1 (yield 0 after transform)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    daraframe = dataframe object variable\n",
    "    first_data_column = first column that contains actual data (first non categorical)\n",
    "    last_data_column = last column taht contains actual data (last non categorigal column)\n",
    "\n",
    "    Run the following to execute the function for Cu transition dataset. \n",
    "\n",
    "    log_2_transform(df, \"5GB1_FM40_T0m_TR2\", \"5GB1_FM40_T180m_TR1\") \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_data = dataframe.loc[:,first_data_column:last_data_column] #isolate the data\n",
    "    \n",
    "    df_data = df_data.replace(0,1) #replace all zeros with 1s\n",
    "    \n",
    "    df_data[df_data<1] = 1 #replace all values less than 1 with 1\n",
    "    \n",
    "    df_data_log2 = df_data.apply(np.log2)\n",
    "    \n",
    "    return df_data_log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean center the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_center(df, first_data_column, last_data_column):       \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    mean_center(dataframe, \n",
    "                first_data_column, \n",
    "                last_data_column)\n",
    "    \n",
    "    Return a new dataframe with the range of data columns log2 transformed. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    daraframe = dataframe object variable\n",
    "    first_data_column = first column that contains actual data (first non categorical)\n",
    "    last_data_column = last column taht contains actual data (last non categorigal column)\n",
    "\n",
    "    Run the following to execute the function for Cu transition dataset. \n",
    "\n",
    "    mean_center(df, \"5GB1_FM40_T0m_TR2\", \"5GB1_FM40_T180m_TR1\") \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df2_TPM_values = df2_TPM.loc[:,first_data_column:last_data_column] #isolating the data values \n",
    "    df2_TPM_values_T = df2_TPM_values.T #transposing the data\n",
    "\n",
    "    standard_scaler = StandardScaler(with_std=False)\n",
    "    TPM_counts_mean_centered = standard_scaler.fit_transform(df2_TPM_values_T) #mean centering the data \n",
    "\n",
    "    TPM_counts_mean_centered = pd.DataFrame(TPM_counts_mean_centered) #back to Dataframe\n",
    "\n",
    "    #transposing back to original form and reincerting indeces and columns \n",
    "    my_index = df2_TPM_values.index\n",
    "    my_columns = df2_TPM_values.columns\n",
    "\n",
    "    TPM_counts_mean_centered = TPM_counts_mean_centered.T\n",
    "    TPM_counts_mean_centered.set_index(my_index, inplace=True)\n",
    "    TPM_counts_mean_centered.columns = my_columns\n",
    "    \n",
    "    return TPM_counts_mean_centered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pariwise distance metric table - euclidean distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(dataframe, first_data_column, last_data_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    euclidean_distance(dataframe, \n",
    "                first_data_column, \n",
    "                last_data_column)\n",
    "    \n",
    "    Return a new dataframe - pairwise distance metric table, euclidean distance between every pair of rows. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    daraframe = dataframe object variable\n",
    "    first_data_column = first column that contains actual data (first non categorical)\n",
    "    last_data_column = last column taht contains actual data (last non categorigal column)\n",
    "\n",
    "    Run the following to execute the function for Cu transition dataset. \n",
    "\n",
    "    euclidean_distance(df, \"5GB1_FM40_T0m_TR2\", \"5GB1_FM40_T180m_TR1\") \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df_values = dataframe.loc[:,first_data_column:last_data_column] #isolating the data values \n",
    "\n",
    "    df_euclidean_distance = pd.DataFrame(euclidean_distances(df_values))\n",
    "\n",
    "    my_index = dataframe.index\n",
    "    \n",
    "    df_euclidean_distance = df_euclidean_distance.set_index(my_index)\n",
    "    df_euclidean_distance.columns = my_index\n",
    "    \n",
    "    return df_euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def congruency_table(df, \n",
    "         data_clm_strt, \n",
    "         data_clm_stop, \n",
    "         step, \n",
    "         mask_diagonal=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    corr(df, data_clm_strt, data_clm_stop, step = len(df.columns), mask_diagonal=False)\n",
    "    \n",
    "    returns a new datafram - congruency table - a pairwise pearson correlation matrix for every row pair\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df - dataframe argument - recommended to use TPM counts for RNAseq datasets. \n",
    "    data_clm_strt = first column that contains data to be processed \n",
    "    data_clm_stop = last column that contains data to be processed\n",
    "    step = length of dataset \n",
    "    mask_diagonal = mask diagonal values which shoud come out as 1\n",
    "    \n",
    "    \n",
    "    Run the following lines to execute the function for my data\n",
    "    \n",
    "    congruency_table(df1, \"5GB1_FM40_T0m_TR2\" , \"5GB1_FM40_T180m_TR1\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.loc[:, data_clm_strt: data_clm_stop] #isolating the rows that are relavent to us. \n",
    "    df = df.T \n",
    "    \n",
    "    n = df.shape[0]\n",
    "\n",
    "    def corr_closure(df):\n",
    "        d = df.values\n",
    "        sums = d.sum(0, keepdims=True)\n",
    "        stds = d.std(0, keepdims=True)\n",
    "\n",
    "        def corr_(k=0, l=10):\n",
    "            d2 = d.T.dot(d[:, k:l])\n",
    "            sums2 = sums.T.dot(sums[:, k:l])\n",
    "            stds2 = stds.T.dot(stds[:, k:l])\n",
    "\n",
    "            return pd.DataFrame((d2 - sums2 / n) / stds2 / n,\n",
    "                                df.columns, df.columns[k:l])\n",
    "\n",
    "        return corr_\n",
    "\n",
    "    c = corr_closure(df)\n",
    "\n",
    "    step = min(step, df.shape[1])\n",
    "\n",
    "    tups = zip(range(0, n, step), range(step, n + step, step))\n",
    "\n",
    "    corr_table = pd.concat([c(*t) for t in tups], axis=1)\n",
    "\n",
    "    if mask_diagonal:\n",
    "        np.fill_diagonal(corr_table.values, np.nan)\n",
    "\n",
    "    return corr_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5G_counts.tsv was located in the directory\n",
      "5G_counts.tsv was imported into dataframe\n",
      "QC columns were removed from dataframe\n",
      "All non FM40 data were removed from dataframe\n",
      "All FM40 columns were sorted by timecourse sequence\n",
      "Clean-up of raw data complete\n",
      "The shape of the TPM table is  (4593, 16)\n",
      "The shape of the pearson_r matrix is  (4593, 4593)\n"
     ]
    }
   ],
   "source": [
    "df1_raw_FM40 = raw_data_cleanup(\"5G_counts.tsv\")\n",
    "\n",
    "\n",
    "columns = ['5GB1_FM40_T0m_TR2', '5GB1_FM40_T10m_TR3', '5GB1_FM40_T20m_TR2', '5GB1_FM40_T40m_TR1',\n",
    "           '5GB1_FM40_T60m_TR1', '5GB1_FM40_T90m_TR2', '5GB1_FM40_T150m_TR1_remake', '5GB1_FM40_T180m_TR1']\n",
    "\n",
    "df2_TPM = TPM_counts(df1_raw_FM40, \"start_coord\", \"end_coord\",columns)  #TPM counts\n",
    "df2_TPM_log2 = log_2_transform(df2_TPM, \"5GB1_FM40_T0m_TR2\",\"5GB1_FM40_T180m_TR1\") #TPM log 2 transformed \n",
    "df2_TPM_mean = mean_center(df2_TPM, \"5GB1_FM40_T0m_TR2\",\"5GB1_FM40_T180m_TR1\") #TPM mean centered \n",
    "\n",
    "df3_pearson_r = congruency_table(df2_TPM, \"5GB1_FM40_T0m_TR2\" , \"5GB1_FM40_T180m_TR1\", step = df2_TPM.shape[0])\n",
    "df3_euclidean_mean = euclidean_distance(df2_TPM_mean, \"5GB1_FM40_T0m_TR2\" , \"5GB1_FM40_T180m_TR1\")\n",
    "df3_euclidean_log2 = euclidean_distance(df2_TPM_mean, \"5GB1_FM40_T0m_TR2\" , \"5GB1_FM40_T180m_TR1\" )\n",
    "\n",
    "print(\"The shape of the TPM table is \", df2_TPM.shape)\n",
    "print(\"The shape of the pearson_r matrix is \", df3_pearson_r.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson R table gives N/A values for zeros\n",
    "\n",
    "How to go about this, how many values does this apply to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_pearson_r[0:1].isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 94 N/A vlaues (0 gene expression). If there is zero gene expression that means a zero correlation, going to manually change these values to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3_pearson_r_noNA = df3_pearson_r.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ceating a smaller dataframe with some null values\n",
    "df_test = df3_pearson_r.iloc[4590:,4590:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>locus_tag</th>\n",
       "      <th>MBURv2_tRNA7</th>\n",
       "      <th>MBURv2_tRNA8</th>\n",
       "      <th>MBURv2_tRNA9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locus_tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605269</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA8</th>\n",
       "      <td>0.605269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "locus_tag     MBURv2_tRNA7  MBURv2_tRNA8  MBURv2_tRNA9\n",
       "locus_tag                                             \n",
       "MBURv2_tRNA7      1.000000      0.605269           NaN\n",
       "MBURv2_tRNA8      0.605269      1.000000           NaN\n",
       "MBURv2_tRNA9           NaN           NaN           NaN"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>locus_tag</th>\n",
       "      <th>MBURv2_tRNA7</th>\n",
       "      <th>MBURv2_tRNA8</th>\n",
       "      <th>MBURv2_tRNA9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locus_tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA8</th>\n",
       "      <td>0.605269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "locus_tag     MBURv2_tRNA7  MBURv2_tRNA8  MBURv2_tRNA9\n",
       "locus_tag                                             \n",
       "MBURv2_tRNA7      1.000000      0.605269             0\n",
       "MBURv2_tRNA8      0.605269      1.000000             0\n",
       "MBURv2_tRNA9      0.000000      0.000000             0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>locus_tag</th>\n",
       "      <th>MBURv2_tRNA7</th>\n",
       "      <th>MBURv2_tRNA8</th>\n",
       "      <th>MBURv2_tRNA9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locus_tag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MBURv2_tRNA9</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "locus_tag    MBURv2_tRNA7 MBURv2_tRNA8 MBURv2_tRNA9\n",
       "locus_tag                                          \n",
       "MBURv2_tRNA7        False        False         True\n",
       "MBURv2_tRNA8        False        False         True\n",
       "MBURv2_tRNA9         True         True         True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to cluster 5.757973909378052\n"
     ]
    }
   ],
   "source": [
    "# Clustering the pearsons_R with N/A vlaues removed \n",
    "\n",
    "hdb_t1 = time.time()\n",
    "hdb_pearson_r = hdbscan.HDBSCAN(metric = \"precomputed\", min_cluster_size=10).fit(df3_pearson_r_noNA)\n",
    "hdb_pearson_r_labels = hdb.labels_\n",
    "hdb_elapsed_time = time.time() - hdb_t1\n",
    "print(\"time to cluster\", hdb_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(hdb_pearson_r_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 67])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(hdb_pearson_r_labels[hdb_pearson_r_labels!=-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import time\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to cluster 4.468245983123779\n"
     ]
    }
   ],
   "source": [
    "# Clustering the mean centered euclidean distance of TPM counts \n",
    "\n",
    "hdb_t1 = time.time()\n",
    "hdb_euclidean_mean = hdbscan.HDBSCAN(metric = \"precomputed\", min_cluster_size=10).fit(df3_euclidean_mean)\n",
    "hdb_euclidean_mean_labels = hdb.labels_\n",
    "hdb_elapsed_time = time.time() - hdb_t1\n",
    "print(\"time to cluster\", hdb_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(hdb_euclidean_mean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 67])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(hdb_euclidean_mean_labels[hdb_euclidean_mean_labels!=-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to convert dataframe to numpy array and see if the result is the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array3_euclidean_mean=np.array(df3_euclidean_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to cluster 4.111801862716675\n"
     ]
    }
   ],
   "source": [
    "hdb_t1 = time.time()\n",
    "hdb_array_euclidean_mean = hdbscan.HDBSCAN(metric = \"precomputed\", min_cluster_size=10).fit(array3_euclidean_mean)\n",
    "hdb_array_euclidean_mean_labels = hdb.labels_\n",
    "hdb_elapsed_time = time.time() - hdb_t1\n",
    "print(\"time to cluster\", hdb_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 67])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(hdb_array_euclidean_mean_labels[hdb_array_euclidean_mean_labels!=-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like wether it is a numpy array or pandas dataframe, the result is the same. lets now try to get index of the clustered points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HDBSCAN' object has no attribute 'n_clusters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-4dc44f2b99ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdb_array_euclidean_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdb_array_euclidean_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'HDBSCAN' object has no attribute 'n_clusters'"
     ]
    }
   ],
   "source": [
    "{i: np.where(hdb_array_euclidean_mean.labels_ == i)[0] for i in range(hdb_array_euclidean_mean.n_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to cluster 3.162208080291748\n"
     ]
    }
   ],
   "source": [
    "# Clustering the log2 transformed euclidean distance of TPM counts \n",
    "\n",
    "hdb_t1 = time.time()\n",
    "hdb_euclidean_log2 = hdbscan.HDBSCAN(metric = \"precomputed\", min_cluster_size=10).fit(df3_euclidean_log2)\n",
    "hdb_euclidean_log2_labels = hdb.labels_\n",
    "hdb_elapsed_time = time.time() - hdb_t1\n",
    "print(\"time to cluster\", hdb_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(hdb_euclidean_log2_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 67])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(hdb_euclidean_log2_labels[hdb_euclidean_log2_labels!=-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following along to figure out the indices of my clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimator = KMeans(n_clusters=3)\n",
    "estimator.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2,\n",
       "       0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ 52,  77, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112,\n",
       "        115, 116, 117, 118, 120, 122, 124, 125, 128, 129, 130, 131, 132,\n",
       "        134, 135, 136, 137, 139, 140, 141, 143, 144, 145, 147, 148]),\n",
       " 1: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n",
       " 2: array([ 50,  51,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "         64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 101, 106, 113, 114,\n",
       "        119, 121, 123, 126, 127, 133, 138, 142, 146, 149])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i: np.where(estimator.labels_ == i)[0] for i in range(estimator.n_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
